package ksb.csle.component.reader

import scala.collection.JavaConversions._

import com.mongodb.spark._
import com.mongodb.spark.config._

import org.apache.spark.sql._
import org.apache.spark.sql.types._

import ksb.csle.common.proto.DatasourceProto.{ReaderInfo, BatchReaderInfo}
import ksb.csle.common.base.reader.BaseReader
import ksb.csle.common.utils.SparkUtils

import ksb.csle.component.ingestion.common._

/**
 * :: ApplicationDeveloperApi ::
 *
 * Reader that loads records from MongoDB.
 *
 * @param o Object that contains message
 *          [[ksb.csle.common.proto.DatasourceProto.MongodbInfo]]
 *          MongodbInfo contains attributes as follows:
 *          - serverAddress: MongoDB address
 *                           (e.g. 192.168.0.3:27017) (required)
 *          - dbName: Database name to access (required)
 *          - collectionName: Collection name to access (required)
 *          - userName: User name if requires authentication (optional)
 *          - password: Password if requires authentication (optional)
 *          - cleaningDataInconsistency: Clean the record if has
 *                                       inconsistent data (optional)
 *          - field: Schema of input record (optional)
 *
 * ==MongodbInfo==
 * {{{
 * message MongodbInfo {
 *   required string serverAddress = 1;
 *   required string dbName = 2;
 *   required string collectionName = 3;
 *   optional string userName = 4;
 *   optional string password = 5;
 *   optional bool cleaningDataInconsistency = 6;
 *   repeated FieldInfo field = 7;
 * }
 * }}}
 */
class MongodbReader(
    val o: BatchReaderInfo
    ) extends BaseReader[DataFrame, BatchReaderInfo, SparkSession](o) {

  import MongodbReader._

  private[this] val info =
    if (o.getMongodbReader == null) {
      throw new IllegalArgumentException("MongodbInfo is not set.")
    } else {
      o.getMongodbReader
    }

  private[this] var mongoSpark: MongoSpark = null

  /**
   * Load records in as DataFrame from MongoDB.
   *
   * @param  session   Spark session
   * @return DataFrame DataFrame read from MongoDB
   */
  @throws(classOf[Exception])
  override def read(session: SparkSession): Dataset[Row] = {
    val readConfig = ReadConfig(Map(
        "uri" -> s"mongodb://${info.getServerAddress}",
        "database" -> info.getDbName,
        "collection" -> info.getCollectionName))

    mongoSpark = MongoSpark.builder()
      .sparkSession(session)
      .readConfig(readConfig)
      .build()

    if (info.getFieldCount() > 0) {
      val schema = SparkUtils.getSchema(info.getFieldList().toList)
      if (info.getCleaningDataInconsistency() == true) {
        createCleanDataFrame(mongoSpark, schema)
      } else {
        mongoSpark.toDF(schema)
      }
    } else {
      // drop the '_id' field generated by MongoDB automatically.
      mongoSpark.toDF().drop("_id")
    }
  }

  /**
   * Close the connection with MongoDB.
   */
  @throws(classOf[Exception])
  override def close {
    if (mongoSpark != null) {
      mongoSpark.connector.close()
    }
  }
}

object MongodbReader {
  def apply(o: BatchReaderInfo): MongodbReader = new MongodbReader(o)

  private def createCleanDataFrame(mongoSpark: MongoSpark,
      schema: StructType): DataFrame = {
    val mongoRDD = mongoSpark.toRDD()

    val rowRDD = mongoRDD.map { doc =>
      val cleaned = schema.fields.map { f =>
        val v = doc.get(f.name)
        if (v == null) v else cleanIfInconsistent(f.dataType, v)
      }
      Row.fromSeq(cleaned)
    }

    mongoSpark.sparkSession.createDataFrame(rowRDD, schema)
  }

  private def cleanIfInconsistent(dataType: DataType, value: Object): Any = {
    (dataType, value) match {
      case (IntegerType, n: java.lang.Number) => n.intValue()
      case (IntegerType, _) => null
      case (DoubleType, n: java.lang.Number) => n.doubleValue()
      case (DoubleType, _) => null
      case (BooleanType, b: java.lang.Boolean) => b
      case (BooleanType, _) => null
      case (ByteType, n: java.lang.Number) => n.byteValue()
      case (ByteType, _) => null
      case (FloatType, n: java.lang.Number) => n.floatValue()
      case (FloatType, _) => null
      case (LongType, n: java.lang.Number) => n.longValue()
      case (LongType, _) => null
      case (StringType, s: java.lang.String) => s
      case (StringType, _) => value.toString()
      case (TimestampType, t: java.sql.Timestamp) => t
      case (TimestampType, _) => null
      case _ => value
    }
  }
}
